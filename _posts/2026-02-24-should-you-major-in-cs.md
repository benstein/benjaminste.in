---
layout: default
title: "Should You Major in Computer Science in the Age of AI?"
date: 2026-02-24
categories: [education, ai, engineering]
excerpt: "I failed my first computer science exam. Not 'didn't do great.' Failed. It was the best lessons I ever learned."
---

I failed my first computer science exam. Not "didn't do great." Failed.

It was CS100 at Cornell. I had never written a line of code before college, and my first exposure to programming was C. Actual C. Not Python, not JavaScript, not some friendly language that holds your hand and tells you everything's going to be okay. C. The language where you manage your own memory (or in my case, don't)

So I did what any rational 18-year-old does: I studied syntax like my life depended on it. Where does the semicolon go? How do you nest curly braces? What's the difference between `*ptr`, `**ptr`, and `&ptr`? What's the indentation convention? I had that stuff absolutely cold. I could punctuate C in my sleep.

Then I sat down for the exam and none of it mattered. The questions weren't about syntax. They were about algorithms. They were puzzles. They were "how would you approach this problem?" and "what's the most efficient way to think about this?" I sat there in a state of mild panic until somewhere between questions two and three I had one of those bizarre, inconvenient epipahines: computer science has basically nothing to do with syntax.

I still failed the test. But I walked out thinking this is the most fascinating thing I'd ever experienced in school.

## The Syntax Was Never the Point

If you think engineering is about memorizing syntax, you're making the same mistake I made in 1996. And if you think AI means nobody should study engineering because "LLMs will just write the code," you're making a more sophisticated version of the same mistake.

<aside class="pull-quote"><p>What I actually learned studying computer science had very little to do with semicolons. It had everything to do with how to reason about the world.</p></aside>

I learned about **abstraction**, which is the practice of separating what something does from how it does it. In code, that means interfaces and layers. In real life, that's the plumbing in your house. I don't need to understand fluid dynamics to fix my toilet. I need to understand the boundary between systems. (I'm also pretty bad at fixing my toilet, but that's a separate issue.)

I learned about **algorithmic complexity**, Big O notation, which sounds impressively nerdy and is. But the actual skill it teaches you is thinking in orders of magnitude. Is this problem linear or exponential? If we double the inputs, does it get a little harder or does it explode? That lens applies to companies, markets, families, and the rate at which my 2 teenage boys consume Top Ramen.

I learned about **pointers and references**, which is a mildly metaphysical concept if you think about it too long. The idea that something can *point to* something else rather than *be* the thing itself will genuinely rewire your brain. Indirection isn't intuitive. It's also incredibly powerful once you internalize it.

I learned **divide and conquer**: break a giant problem into composable pieces, make each piece testable on its own, then reassemble them. I learned **debugging**, which sounds like a technical skill but is really the art of forming hypotheses, instrumenting a system, observing what actually happens versus what you expected, and adjusting your mental model accordingly. So when my wife says "why isn't the Netflix sound working?", I can systematically sovle the problem before the new season of Bridgerton starts.

At some point I stopped thinking of these as "skills" and started thinking of them as tools in my proverbial toolbelt. And they compound over time in ways that are hard to appreciate when you're 18 and staring at a failed exam.

And here's the thing: that idea generalizes to basically everything. You just need different glasses. Zoom out. Change representations. Reframe the problem. The discipline teaches you to look at what appears to be total chaos and realize it has hidden order, if you're willing to shift your perspective.

## So... Should You Major in CS in 2026?

OK, to the heart of the matter. AI can write code. Therefore (the argument goes) you should skip computer science because entry-level programming jobs might look different in five years.

If your only reason for going to university is to secure a very specific job title, then maybe that's a reasonable concern. But if you believe the purpose of higher education is to learn how to think, then you're asking the wrong question entirely. University is not trade school. (And before anyone sharpens their pitchforks: trade school is great. Electricians, plumbers, machinists, these are real skills that keep civilization functioning. My house would collapse without them, possibly literally. This isn't a referendum on trades. It's about what a rigorous academic discipline uniquely offers.)

Studying engineering, or philosophy, or physics, or classics isn't about landing a specific job. You go in thinking one way and come out thinking differently, with a set of mental tools you didn't have before and can't easily acquire any other way.

## So... Should You Major in Humanities Instead?

The idea of learning how to think differently is not unique to engineering, of course. Consider a classics major. They spend years reading ancient texts, parsing dead languages, studying historical context, writing, arguing. What do they actually learn? They learn to interpret ambiguous material, construct and dismantle arguments, trace how ideas evolve across centuries, and write with the kind of clarity and precision that is, frankly, a superpower most people underestimate.

Those aren't "job skills" in the narrow LinkedIn sense. They're thinking patterns. A philosophy major learns to reason formally. A history major learns to trace causality across complex systems. A literature major learns to inhabit other minds, which is honestly one of the most underrated abilities a person can develop.

None of these disciplines are "about" employability in any direct way. They're about building a different brain. Engineering does the same thing, just with algorithms instead of Kafka. (OK maybe a bad example)

## The Skills We'll Need in the Age of AI

We don't know which specific skills will be most valuable in 2035. Maybe it's writing. Maybe it's prioritization. Maybe it's taste. Maybe it's interpersonal communication. Maybe it's something we can't even name yet because the job doesn't exist.

The ability to think clearly, abstractly, structurally, and creatively is not going anywhere. Pattern matching, decomposition, adaptation, model-building, debugging your own assumptions (which, if we're being honest, is the hardest kind of debugging there is).

AI will generate code. It will draft essays. It will design interfaces. But someone still needs to define the problem worth solving, evaluate tradeoffs, recognize when the output is subtly wrong (which requires understanding why it should be right), and design the system that ties it all together.

Thinking doesn't get automated away. If anything, the ability to think well becomes *more* valuable when the cost of generating mediocre output drops to zero. The bottleneck shifts from production to judgment.

## Follow the Field That Rewires Your Brain. And Your Passion.

Through my nonprofit work with [Piedmont Makers](https://piedmontmakers.org), I get to help inspire so many young people to become creative problem solvers and innovators in STEAM. So I certainly get asked questions like this a lot.

If you're choosing a major, I wouldn't optimize for "what job will this get me?" That's a short time horizon in a world that's changing this fast, and it's also kind of a depressing way to pick something you'll spend four years of your life on.

Instead, ask yourself: What field excites me enough that I'll push through the hard parts? What discipline forces me to think in ways I currently don't? What intellectual tools do I want in my belt for the next fifty years?

For me, engineering did that. Failing that first exam was the gateway drug. Once I understood that the whole discipline was really about problem-solving and systems thinking disguised as a typing class, I was hooked.

For someone else, it might be philosophy. Or economics. Or physics. Or literature. The specific content matters less than people think. What matters is whether the discipline grabs you hard enough to drag you through the difficulty, and whether you come out the other side thinking differently than when you went in.

## We're Only Human After All

If AI writes code, great. Let it. I'll be over here with my Big O notation and my slightly traumatic memories of that first CS exam, thinking about systems. Because the real value of studying engineering was never the semicolons. It was learning how to see complexity, decompose it into parts, and reassemble it into something better.

That kind of thinking isn't threatened by AI. If anything, it's exactly what you need to use AI well. The machines got really good at typing. Turns out the being human part is still on us.

Oh, and in case you're wondering, no, I still can't dereference a pointer.
